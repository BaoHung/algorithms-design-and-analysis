Week 2
-----------------------------
IV. THE MASTER METHOD
-----------------------------

Recurrence Format:
------------------
1) Base case: T(n) <= c constant for all sufficiently small n
2) For all larger n: T(n) <= aT(n/b) + O(nˆd) where
    a = number of recursive calls (>= 1)
    b = input size shrinkage factor (> 1)
    d = exponent in running time of "combine step" (>= 0)
    [a, b, d independent of n]

The Master Method:
------------------
T(n) =
    O(nˆdlogn)  if a = bˆd (case 1)
    O(nˆd)      if a < bˆd (case 2)
    O(nˆlogb(a))if a > bˆd (case 3)

Example:
--------
#1 Merge sort:
    a = 2
    b = 2
    d = 1
=> case 1

#2 Binary search in a sorted array:
    a = 1
    b = 2
    d = 0
=> case 1

#3 Integer multiplication(x * y = 10^n * ac + 10^(n/2) * (ad + bc) + bd)
    a = 4
    b = 2
    d = 1
=> case 3

#4 Gauss's integer multiplication (ad + bc = (a + b) * (c + d) - ac - bd)
    a = 3
    b = 2
    d = 1
=> case 3

#5 Strassen's matrix multiplication algorithm:
    a = 7
    b = 2
    d = 2
=> case 3

#6 Fictitious recurrence: T(n) < 2T(n/2) + O(nˆ2)
    a = 2
    b = 2
    d = 2
=> case 2

Proof:
------
Recursion tree:
At each level j = 0,1,2,...,logb(n), there are aˆj subproblems, each of size n/bˆj
Work at level j <= aˆj * c * (n/bˆj)ˆd = cnˆd * (a/bˆd)ˆj
Total work <= cnˆd * ∑(a/bˆd)ˆj (j = 0~logb(n))
    If a/bˆd != 1, proof by induction

Interpretation:
---------------
    a = rate of subproblem proliferation (RSP)
    bˆd = rate of work shrinkage  per subproblem (RWS)

    - If RSP < RWS, then the amount of work is decreasing with the recursion level j
        => most work at the root [might expect O(nˆd)]
    - If RSP > RWS, then the amount of work is increasing with the recursion level j
        => most work at the leaves [might expect O(#leaves) (aˆlogb(n) = nˆlogb(a))]
    - If RSP = RWS, then the amount of work is the same at every recursion level j
        => [expect O(nˆdlogn)]

-----------------------------
V. QUICKSORT - ALGORITHM
-----------------------------
O(nlogn) time "on average", minimal extra memory needed.
Key idea: partition array around a pivot element
    - linear time, no extra memory
    - reduce problem size

In place implementation:
------------------------
Assume: pivot is the 1st element in array (if not, swap to first)
P|<P|>P|?

Pseudocode for partition:
-------------------------
Partition(A, l, r)
- p := A[l]
- i := l + 1
- for j := l + 1 to r
    - if A[j] < p (if A[j > p, do nothing])
        - swap A[j] and A[i]
        - i++
- swap A[l] and A[i - 1]

Correctness of Quicksort:
-------------------------
Proof by induction:
1) base case: array of length 1
2) inductive step: length >= 2, show if P(k) holds, (all k < n), then P(n) holds
as well
    - let k1, k2 = length of 1st, 2nd parts of partitioned array
    - if 1st, 2nd parts get sorted correctly by recursive calls
    - so after recursive calls, entire array correctly sorted

If input array is already sorted: O(nˆ2)
Random Pivot: average is n(logn)

-----------------------------
VI. QUICKSORT - ANALYSIS
-----------------------------
A General Decomposition Principle
1. Iden8fy random variable Y that you really care about
2. Express Y as sum of indicator random variables:
    Y = ∑Xe (l = 1~m)
3. Apply Linearity of expectation:
    E[Y] = ∑Pr[Xe = 1] (l = 1~m)

Proof of Key Claim:
-------------------
- Fix zi, zj with i < j
- Consider the set zi,zi+1,…,zj-1,zj
- Inductively: as long as none of these are chosen as a pivot, all are passed to
the same recursive call.
- Consider the first among zi,zi+1,..,zj-1,zj that gets chosen as a pivot.
    1. If zi or zj gets chosen first, then zi and zj get compared
    2. If one of zi+1,…,zj-1 gets chosen first then zi and zj are never compared
    [split into different recursive calls]
- Note: Since pivots always chosen uniformly at random, each of zi,zi+1,…,zjI1,zj
is equally likely to be the first
⇒Pr[zi,zj get compared] = 2/(j-i+1)
⇒So: E[C] = ∑(i = 1~n-1)∑(j = 1~n) 2/(j-1+1)
